# ğŸ¡ Projet Data Science â€“ PrÃ©diction des Prix de l'Immobilier en Californie  
*(Dataset : `fetch_california_housing` â€“ scikit-learn)*[1]

## 1. Contexte mÃ©tier et mission

### Le problÃ¨me (Business Case)

En Californie, les acteurs de l'immobilier (agences, promoteurs, banques) doivent estimer rapidement la valeur mÃ©diane des logements par bloc gÃ©ographique pour guider les investissements, fixer les prix de vente et accorder des crÃ©dits hypothÃ©caires.[2]
Une sous-Ã©valuation entraÃ®ne une perte de revenus, tandis qu'une sur-Ã©valuation expose Ã  des risques de dÃ©faut de paiement.[3][2]

**Objectif :** DÃ©velopper un modÃ¨le de rÃ©gression prÃ©disant la valeur mÃ©diane des maisons (en centaines de milliers de dollars) Ã  partir de 8 features socio-dÃ©mographiques et gÃ©ographiques.[1]

### L'enjeu mÃ©tier
- **DÃ©cision d'investissement** : Identifier les zones Ã  fort potentiel.  
- **Gestion du risque bancaire** : Ã‰valuation rÃ©aliste pour les prÃªts immobiliers.  
- **Simulation stratÃ©gique** : Impact du revenu mÃ©dian ou de la densitÃ© sur les prix.[3]

***

## 2. Les donnÃ©es (l'Input)

**Dataset California Housing** (20 640 Ã©chantillons, 8 features numÃ©riques continues).[1]

| Ã‰lÃ©ment | Description |  
|---------|-------------|  
| **Samples** | 20 640 blocs de recensement californiens [1] |  
| **Features (X)** | MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude [2] |  
| **Target (y)** | `MedHouseVal` (valeur mÃ©diane des maisons Ã—100k $) [1] |  

***

## 3. Code Python Complet â€“ Cycle de vie

### 3.1 Importation des bibliothÃ¨ques

```python
# ==============================================================================
# COURS DATA SCIENCE : CYCLE DE VIE COMPLET (SCRIPT PÃ‰DAGOGIQUE)
# PROBLÃˆME DE RÃ‰GRESSION : PRÃ‰DICTION DES PRIX DES MAISONS EN CALIFORNIE
# ==============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-Learn
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Configuration
sns.set_theme(style="whitegrid")
import warnings
warnings.filterwarnings('ignore')
print("âœ… BibliothÃ¨ques importÃ©es.\n")
```

### 3.2 Chargement & simulation donnÃ©es sales

```python
# 2. CHARGEMENT
data = fetch_california_housing(as_frame=True)
df = data.frame
df.rename(columns={'MedHouseVal': 'target'}, inplace=True)
print(f"ğŸ“Š Dataset : {df.shape}")

# 3. SIMULATION DONNÃ‰ES SALES (5% NaN)
np.random.seed(42)
features_columns = df.columns[:-1]
df_dirty = df.copy()
for col in features_columns:
    df_dirty.loc[df_dirty.sample(frac=0.05, random_state=42).index, col] = np.nan
print(f"ğŸ•³ï¸  NaN gÃ©nÃ©rÃ©s : {df_dirty.isnull().sum().sum()}\n")
```

### 3.3 Nettoyage (Data Wrangling + Scaling)

```python
# 4. NETTOYAGE
X = df_dirty.drop('target', axis=1)
y = df_dirty['target']

# A. Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)
print("âœ… Imputation OK")

# B. Scaling (CRUCIAL pour rÃ©gression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clean)
X_clean_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)
print("âœ… Scaling OK\n")
```

**ğŸ’¡ Expert** : En prod, split â†’ fit(imputer/scaler) sur Train â†’ transform Train/Test.[4]

***

## 4. Analyse Exploratoire (EDA)

```python
# 5. EDA
print("ğŸ“ˆ EDA...")

# Stats cible
print("Statistiques target :\n", y.describe())

# Histogramme cible
plt.figure(figsize=(10, 5))
sns.histplot(y, kde=True, bins=50)
plt.title("Distribution Prix Maisons ($100k)")
plt.show()

# CorrÃ©lations avec target
plt.figure(figsize=(10, 8))
corr_matrix = pd.concat([X_clean, y], axis=1).corr()
sns.heatmap(corr_matrix[['target']].sort_values('target', ascending=False),
            annot=True, cmap='coolwarm', fmt=".2f")
plt.title("CorrÃ©lations avec Prix")
plt.show()
```

**Insights** : `MedInc` â‰ˆ +0.7 corrÃ©lation. Distribution skewed (prix plafonnÃ©s).[2]

***

## 5. Split Train/Test

```python
# 6. SPLIT 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X_clean_scaled, y, test_size=0.2, random_state=42
)
print(f"ğŸš‚ Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\n")
```

**random_state=42** = reproductibilitÃ© scientifique.[4]

***

## 6. ModÃ©lisation : RandomForestRegressor ğŸŒ²

```python
# 7. MODÃˆLE
print("ğŸ¤– EntraÃ®nement Random Forest...")
model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)
print("âœ… ModÃ¨le entraÃ®nÃ©\n")
```

**Pourquoi RF ?** RÃ©duit variance (bagging + feature randomness). Robust to outliers.[5]

***

## 7. Ã‰valuation (MÃ©triques RÃ©gression)

```python
# 8. Ã‰VALUATION
y_pred = model.predict(X_test)

# MÃ©triques
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"ğŸ¯ RÂ² : {r2:.4f}")
print(f"ğŸ“ RMSE: {rmse:.4f} ($100k)")

# Scatter plot
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel("RÃ©alitÃ© ($100k)"); plt.ylabel("PrÃ©diction ($100k)")
plt.title('RÃ©alitÃ© vs PrÃ©diction')
plt.show()

print("\nğŸ FIN")
```

**InterprÃ©tation** : RMSE=0.5 â‰ˆ erreur moyenne 50k$. RÂ²>0.8 = excellent pour baseline.[2]

***

## 8. Pipeline Complet â€“ SchÃ©ma

```
ğŸ“¥ DonnÃ©es Sales â†’ ğŸ§¹ Imputation â†’ âš–ï¸ Scaling â†’ ğŸ”€ Split â†’ ğŸŒ² RF â†’ ğŸ“Š MÃ©triques
   (5% NaN)       â†’ (mean)      â†’ (StdScaler) â†’ 80/20  â†’ (100 trees) â†’ RÂ²/RMSE
```

***

## 9. RÃ©capitulatif Technique

| Phase | Outil | RÃ©sultat Attendu |
|-------|-------|------------------|
| **Chargement** | `fetch_california_housing()` | (20640, 9) [1] |
| **Nettoyage** | `SimpleImputer(mean)` | 0 NaN |
| **Scaling** | `StandardScaler()` | Î¼=0, Ïƒ=1 par feature |
| **ModÃ¨le** | `RandomForestRegressor(100)` | RÂ² â‰ˆ 0.80-0.85 [5] |
| **Ã‰val** | RMSE en $100k | <0.55 typique |

***

## 10. Conclusion Projet

Ce pipeline complet transforme un **problÃ¨me mÃ©tier** (Ã©valuation immobiliÃ¨re) en **solution IA actionable** : de l'acquisition Ã  l'Ã©valuation, en passant par un EDA mÃ©tier et un modÃ¨le robuste.[5][2]
**Prochaines Ã©tapes** : Feature engineering (interactions gÃ©o/revenu), GridSearchCV, dÃ©ploiement Streamlit.

[1](https://sklearn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)
[2](https://www.classes.cs.uchicago.edu/archive/2021/fall/12100-1/pa/pa5/dataset-houseprice.html)
[3](https://irays-teknology-ltd.com/BLOG/California-Housing/)
[4](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html)
[5](https://dataloop.ai/library/model/rajistics_california_housing/)
