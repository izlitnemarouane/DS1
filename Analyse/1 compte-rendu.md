# IZLITNE MAROUANE

<img src="IZLITNE MAROUANE.jpg" width="200" align="left" style="margin-right: 20px; border-radius: 10px;"/>

<br>

**Num√©ro d'√©tudiant** : 22006529  
**Classe** : CAC2

<br clear="left"/>

---

# Compte rendu : Analyse Pr√©dictive des Ventes (Business_Sales_EDA)

**Date :** 26 Novembre 2025

---

## √Ä propos du jeu de donn√©es

Le jeu de donn√©es **Business_Sales_EDA**, utilis√© dans cette analyse, recense des transactions de vente d√©taill√©es pour divers produits (v√™tements, chaussures, vestes). Chaque ligne repr√©sente un produit sp√©cifique avec ses caract√©ristiques intrins√®ques et contextuelles.

L'objectif est de pr√©dire la variable cible **Sales Volume** (Volume des ventes) en fonction de divers facteurs marketing et produits tels que :
* **Positionnement** : Emplacement dans le magasin (Aisle, End-cap, Front of Store).
* **Marketing** : Indicateurs de promotion (`Promotion`, `Seasonal`).
* **Caract√©ristiques Produit** : Cat√©gorie, Prix, Marque, Mat√©riau, Origine.

Ce dataset permet d'√©valuer l'impact des strat√©gies de mise en avant et des caract√©ristiques produits sur la performance commerciale.

---

## Table des Mati√®res

1. [Introduction et Contexte](#1-introduction-et-contexte)
2. [Analyse Exploratoire des Donn√©es (EDA)](#2-analyse-exploratoire-des-donn√©es-eda)
    - [2.1 Chargement et Aper√ßu](#21-chargement-et-aper√ßu)
    - [2.2 Pr√©traitement et Encodage](#22-pr√©traitement-et-encodage)
    - [2.3 Analyse des Valeurs Manquantes](#23-analyse-des-valeurs-manquantes)
3. [M√©thodologie de Mod√©lisation](#3-m√©thodologie-de-mod√©lisation)
    - [3.1 S√©paration des Donn√©es (Data Split)](#31-s√©paration-des-donn√©es-data-split)
4. [Impl√©mentation des Mod√®les et R√©sultats](#4-impl√©mentation-des-mod√®les-et-r√©sultats)
    - [4.1 R√©gression Lin√©aire](#41-r√©gression-lin√©aire)
    - [4.2 Arbre de D√©cision (Decision Tree)](#42-arbre-de-d√©cision)
    - [4.3 For√™t Al√©atoire (Random Forest)](#43-for√™t-al√©atoire)
    - [4.4 Support Vector Regressor (SVR)](#44-support-vector-regressor)
    - [4.5 Gradient Boosting Regressor (Le Meilleur Mod√®le)](#45-gradient-boosting-regressor)
5. [Tableau Comparatif et Analyse](#5-tableau-comparatif-et-analyse)
6. [Conclusion](#6-conclusion)

---

## 1. Introduction et Contexte

L'objectif de ce projet est de d√©velopper un mod√®le de machine learning capable de pr√©dire le **Volume des Ventes** ($Y$) avec la plus grande pr√©cision possible. Nous avons compar√© plusieurs algorithmes de r√©gression pour d√©terminer lequel capture le mieux les relations entre les variables explicatives ($X$) et la cible.

La d√©marche suivie est la suivante :
1.  Nettoyage et encodage des donn√©es (traitement des variables cat√©gorielles comme "Promotion" ou "Seasonal").
2.  S√©paration des donn√©es en ensembles d'entra√Ænement et de test.
3.  Entra√Ænement de 5 mod√®les distincts.
4.  Comparaison bas√©e sur le $R^2$ (coefficient de d√©termination), le MAE (erreur absolue moyenne) et le RMSE.

---

## 2. Analyse Exploratoire des Donn√©es (EDA)

### 2.1 Chargement et Aper√ßu

Le dataset est charg√© avec Pandas. Nous observons que le fichier utilise le point-virgule (`;`) comme s√©parateur.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('Business_sales_EDA.csv', sep=';') 
print(f"Dimensions du dataset : {df.shape}")
df.head()
```

### 2.2 Pr√©traitement et Encodage

Les variables telles que `Promotion`, `Seasonal`, et `Product Position` sont de nature **cat√©gorielle**. Nous utilisons le **Label Encoding** pour les transformer en valeurs num√©riques.

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
cat_cols = ['Product Position', 'Promotion', 'Product Category', 'Seasonal', 
            'brand', 'section', 'season', 'material', 'origin']

for col in cat_cols:
    if col in df.columns:
        df[col] = le.fit_transform(df[col].astype(str))

df = df.drop(columns=['url', 'name', 'description', 'currency', 'terms'], errors='ignore')
```

### 2.3 Analyse des Valeurs Manquantes
Avant de mod√©liser, il est essentiel de v√©rifier et de g√©rer les valeurs manquantes (NaN). Nous effectuons une v√©rification, puis nous proc√©dons √† une suppression simple des lignes contenant des valeurs manquantes (df.dropna()) pour garantir l'int√©grit√© des donn√©es d'entra√Ænement.
```python
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])
df = df.dropna()
```

---

## 3. M√©thodologie de Mod√©lisation

### 3.1 S√©paration des Donn√©es (Data Split)
Nous s√©parons nos donn√©es en deux ensembles pour √©valuer la capacit√© de g√©n√©ralisation des mod√®les :

Train (80%) : Utilis√© pour l'entra√Ænement.

Test (20%) : Utilis√© pour l'√©valuation finale.

```python
from sklearn.model_selection import train_test_split

X = df.drop(columns=['Sales Volume', 'Product ID'])
y = df['Sales Volume']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Taille de l'ensemble d'entra√Ænement X : {X_train.shape}")
print(f"Taille de l'ensemble de test X : {X_test.shape}")
```

---

## 4. Impl√©mentation des Mod√®les et R√©sultats

### 4.1 R√©gression Lin√©aire

La R√©gression Lin√©aire cherche une relation lin√©aire directe. Elle sert de mod√®le de base pour √©valuer la performance initiale

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

print("R2 Score:", r2_score(y_test, y_pred_lr))
print("MAE:", mean_absolute_error(y_test, y_pred_lr))
```
**R√©sultats :**

  - **$R^2$ ‚âà 0.93** (93%)
  - **MAE ‚âà 62.39 $**
    
### 4.2 Arbre de D√©cision

L'Arbre de D√©cision capture les relations non-lin√©aires par des divisions conditionnelles successives. Il est rapide mais sujet au sur-apprentissage.

```python
from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

print("R2 Score:", r2_score(y_test, y_pred_dt))
print("MAE:", mean_absolute_error(y_test, y_pred_dt))
```
**R√©sultats :**

  - **$R^2$ ‚âà 0.87** (87%)
  - **MAE ‚âà 82.46 $**
    
### 4.3 For√™t Al√©atoire

Le Random Forest utilise un ensemble de nombreux Arbres de D√©cision et fait la moyenne de leurs pr√©dictions, ce qui r√©duit la variance et am√©liore la robustesse.

```python
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("R2 Score:", r2_score(y_test, y_pred_rf))
print("MAE:", mean_absolute_error(y_test, y_pred_rf))
```
**R√©sultats :**

  - **$R^2$ ‚âà 0.93** (93%)
  - **MAE ‚âà 63.04 $**

### 4.4 Support Vector Regressor

Le SVR cherche √† d√©finir un hyperplan optimal avec une marge d'erreur tol√©r√©e. Il est crucial de scaler les donn√©es au pr√©alable pour ce mod√®le, car il est sensible √† l'√©chelle.

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svr_model = SVR(kernel='rbf')
svr_model.fit(X_train_scaled, y_train)
y_pred_svr = svr_model.predict(X_test_scaled)

print("R2 Score:", r2_score(y_test, y_pred_svr))
print("MAE:", mean_absolute_error(y_test, y_pred_svr))
```
**R√©sultats :**

  - **$R^2$ ‚âà 0.63** (63%)
  - **MAE ‚âà 137.61 $**
  - **MSE ‚âà 32147.57**
  - **RMSE ‚âà 179.30**
    
### 4.5 Gradient Boosting Regressor

Le Gradient Boosting construit les arbres s√©quentiellement. Chaque nouvel arbre est entra√Æn√© pour corriger les erreurs r√©siduelles faites par l'ensemble des arbres pr√©c√©dents, aboutissant souvent √† une pr√©cision sup√©rieure.

```python
from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

r2_gb = r2_score(y_test, y_pred_gb)
mae_gb = mean_absolute_error(y_test, y_pred_gb)
mse_gb = mean_squared_error(y_test, y_pred_gb)
rmse_gb = np.sqrt(mse_gb)

print("R2 Score:", r2_gb)
print("MAE:", mae_gb)
print("RMSE:", rmse_gb)
```
**R√©sultats :**

  - **$R^2$ ‚âà 0.94** (94%)
  - **MAE ‚âà 59.08 $**
  - **MSE ‚âà 5675.85**
  - **RMSE ‚âà 75.34**
    
---

## 5. Tableau Comparatif et Analyse

| Mod√®le | R¬≤ | MAE | MSE | RMSE | Performance |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Gradient Boosting | 0.94 | 59.08 | 5675.85 | 75.34 | üèÜ Meilleur |
| R√©gression Lin√©aire | 0.93 | 62.39 | - | - | ‚≠ê Tr√®s Bon |
| Random Forest | 0.93 | 63.04 | - | - | ‚≠ê Tr√®s Bon |
| Decision Tree | 0.87 | 82.46 | - | - | Moyen |
| SVR | 0.63 | 137.61 | 32147.57 | 179.30 | Faible |

### Analyse des R√©sultats et Recommandations

1.  **Mod√®le Optimal** : Le **Gradient Boosting Regressor** est le plus performant, expliquant 94% de la variance des ventes ($R^2=0.94$) avec l'erreur moyenne la plus faible (MAE=59.08).
2.  **Robustesse** : Les mod√®les bas√©s sur l'ensemble d'arbres (Gradient Boosting et Random Forest) et la R√©gression Lin√©aire offrent les meilleurs r√©sultats, sugg√©rant que les donn√©es contiennent √† la fois des relations lin√©aires et complexes.
3.  **Prochaines √âtapes** : Il est recommand√© de proc√©der √† une optimisation fine des hyperparam√®tres (via GridSearchCV ou RandomizedSearchCV) pour le Gradient Boosting afin de maximiser la performance et d'assurer une meilleure g√©n√©ralisation.

-----

## 6\. Conclusion

Cette analyse pr√©dictive des ventes a d√©montr√© l'efficacit√© des m√©thodes d'ensemble pour mod√©liser le volume des ventes. Le mod√®le **Gradient Boosting Regressor** fournit une base robuste pour la pr√©vision des ventes futures. Ces r√©sultats peuvent directement informer les d√©cisions strat√©giques, telles que l'allocation des budgets marketing ou le positionnement des produits, en quantifiant l'impact des diff√©rentes caract√©ristiques sur les revenus.

```
```
