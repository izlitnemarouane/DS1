
# IZLITNE MAROUANE

<img src="https://image2url.com/images/1765362786985-df3bb0b1-e113-40f7-a0cc-80d894c711cb.jpg"
     alt="Logo marouane izlitne"
     style="height:300px; margin-right:300px; float:left; border-radius:10px;">

<br><br clear="left"/>



**NumÃ©ro d'Ã©tudiant** : 22006529  
**Classe** : CAC2

# VidÃ©o de prÃ©sentation : https://drive.google.com/file/d/1jYMS5vVjKU5KlbxXeOxNK072qVyZ89WM/view?usp=sharing

# ğŸ¡ Projet Data Science â€“ PrÃ©diction des Prix de l'Immobilier en Californie  
*(Dataset : `fetch_california_housing` â€“ scikit-learn)*[1]

## Table des MatiÃ¨res

- [1. Contexte mÃ©tier et mission](#1-contexte-mÃ©tier-et-mission)
- [Les donnÃ©es (l'Input)](#les-donnÃ©es-linput)
- [2. Le Code Python (Laboratoire)](#2-le-code-python-laboratoire)
- [3. Analyse Approfondie : Nettoyage (Data Wrangling)](#3-analyse-approfondie--nettoyage-data-wrangling)
- [4. Analyse Approfondie : Exploration (EDA)](#4-analyse-approfondie--exploration-eda)
- [5. Analyse Approfondie : MÃ©thodologie (Split)](#5-analyse-approfondie--mÃ©thodologie-split)
- [6. FOCUS THÃ‰ORIQUE : Lâ€™Algorithme Random Forest](#6-focus-thÃ©orique--lalgorithme-random-forest-)
- [7. Analyse Approfondie : Les MÃ©triques de RÃ©gression](#7-analyse-approfondie-)
- [8. Pipeline Complet â€“ SchÃ©ma](#8-pipeline-complet--schÃ©ma)
- [9. RÃ©capitulatif Technique](#9-rÃ©capitulatif-technique)
- [Conclusion du Projet](#conclusion-du-projet)

## 1. Contexte mÃ©tier et mission

### Le problÃ¨me (Business Case)

En Californie, les acteurs de l'immobilier (agences, promoteurs, banques) doivent estimer rapidement la valeur mÃ©diane des logements par bloc gÃ©ographique pour guider les investissements, fixer les prix de vente et accorder des crÃ©dits hypothÃ©caires.[2]
Une sous-Ã©valuation entraÃ®ne une perte de revenus, tandis qu'une sur-Ã©valuation expose Ã  des risques de dÃ©faut de paiement.[3][2]

**Objectif :** DÃ©velopper un modÃ¨le de rÃ©gression prÃ©disant la valeur mÃ©diane des maisons (en centaines de milliers de dollars) Ã  partir de 8 features socio-dÃ©mographiques et gÃ©ographiques.[1]

### L'enjeu mÃ©tier
- **DÃ©cision d'investissement** : Identifier les zones Ã  fort potentiel.  
- **Gestion du risque bancaire** : Ã‰valuation rÃ©aliste pour les prÃªts immobiliers.  
- **Simulation stratÃ©gique** : Impact du revenu mÃ©dian ou de la densitÃ© sur les prix.[3]

***

##  Les donnÃ©es (l'Input)

**Dataset California Housing** (20 640 Ã©chantillons, 8 features numÃ©riques continues).[1]

| Ã‰lÃ©ment | Description |  
|---------|-------------|  
| **Samples** | 20 640 blocs de recensement californiens [1] |  
| **Features (X)** | MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude [2] |  
| **Target (y)** | `MedHouseVal` (valeur mÃ©diane des maisons Ã—100k $) [1] |  

***

## 2. Le Code Python (Laboratoire)
```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Configuration
sns.set_theme(style="whitegrid")
import warnings
warnings.filterwarnings('ignore')

# --- PHASE 1 : ACQUISITION & SIMULATION ---
housing = fetch_california_housing(as_frame=True)
df = housing.frame.copy()   # Features + target dÃ©jÃ  combinÃ©s
df.rename(columns={'MedHouseVal': 'target'}, inplace=True)

print(df.head())
print(df.shape)

# Simulation de la rÃ©alitÃ© (DonnÃ©es sales)
np.random.seed(42)
df_dirty = df.copy()

# On corrompt 5% des donnÃ©es de chaque feature avec des NaN
feature_cols = [c for c in df_dirty.columns if c != 'target']
for col in feature_cols:
    df_dirty.loc[df_dirty.sample(frac=0.05, random_state=42).index, col] = np.nan

print("Nombre total de valeurs manquantes gÃ©nÃ©rÃ©es :",
      df_dirty.isnull().sum().sum())

# --- PHASE 2 : DATA WRANGLING (NETTOYAGE) ---
X = df_dirty.drop('target', axis=1)
y = df_dirty['target']

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)

print("Imputation terminÃ©e.")
print("Valeurs manquantes restantes :", X_clean.isnull().sum().sum())

# --- PHASE 3 : ANALYSE EXPLORATOIRE (EDA) ---
print("--- Statistiques Descriptives ---")
print(X_clean.describe())

# Exemple de visualisation 1 : Revenu mÃ©dian vs Prix
plt.figure(figsize=(8, 5))
sns.scatterplot(x=X_clean['MedInc'], y=y, alpha=0.3)
plt.title("Relation entre Revenu MÃ©dian (MedInc) et Prix moyen des maisons")
plt.xlabel("MedInc (Revenu mÃ©dian)")
plt.ylabel("Valeur moyenne des maisons (target)")
plt.show()

# Exemple de visualisation 2 : Matrice de corrÃ©lation
plt.figure(figsize=(10, 8))
corr = pd.concat([X_clean, y], axis=1).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matrice de CorrÃ©lation (Features + cible)")
plt.show()

# --- PHASE 4 : PROTOCOLE EXPÃ‰RIMENTAL (SPLIT) ---
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y, test_size=0.2, random_state=42
)

print("\nSÃ©paration effectuÃ©e :")
print(f"EntraÃ®nement : {X_train.shape[0]} Ã©chantillons")
print(f"Test        : {X_test.shape[0]} Ã©chantillons")

# --- PHASE 5 : INTELLIGENCE ARTIFICIELLE (RANDOM FOREST REGRESSOR) ---
model = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    n_jobs=-1
)
model.fit(X_train, y_train)

# --- PHASE 6 : AUDIT DE PERFORMANCE ---
y_pred = model.predict(X_test)

from math import sqrt
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\n>>> MSE  : {mse:.3f}")
print(f">>> RMSE : {rmse:.3f}")
print(f">>> MAE  : {mae:.3f}")
print(f">>> RÂ²   : {r2:.3f}")

# Visualisation : PrÃ©dictions vs RÃ©alitÃ©
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--', label="IdÃ©al"
)
plt.xlabel("Valeurs rÃ©elles (y_test)")
plt.ylabel("PrÃ©dictions (y_pred)")
plt.title("Random Forest - PrÃ©dictions vs RÃ©alitÃ© (California Housing)")
plt.legend()
plt.show()

```

---

## 3. Analyse Approfondie : Nettoyage (Data Wrangling)

### Le ProblÃ¨me MathÃ©matique du "Vide"
Les algorithmes dâ€™algÃ¨bre linÃ©aire utilisÃ©s par les modÃ¨les de rÃ©gression ne peuvent pas gÃ©rer la valeur NaN (Not a Number). Une seule valeur manquante dans une des features peut faire Ã©chouer lâ€™entraÃ®nement ou fausser complÃ¨tement les calculs de distances, de moyennes ou de splits dans les arbres.

### La MÃ©canique de lâ€™Imputation
Nous utilisons SimpleImputer(strategy='mean').

1.  **Lâ€™Apprentissage (fit) :**  
    Lâ€™imputer scanne par exemple la colonne MedInc pour toutes les zones, calcule la moyenne du revenu mÃ©dian, et stocke cette valeur. Il fait de mÃªme pour chaque feature (AveRooms, Population, etc.).

2.  **La Transformation (transform) :**  
    Lors de la transformation, dÃ¨s quâ€™un trou (NaN) est rencontrÃ© dans une colonne, il est remplacÃ© par la moyenne calculÃ©e Ã  lâ€™Ã©tape prÃ©cÃ©dente pour cette colonne.

Au final, X_clean est une version â€œcomplÃ¨teâ€ du dataset, sans valeurs manquantes, compatible avec les algorithmes de Machine Learning.

### ğŸ’¡ Le Coin de lâ€™Expert (Data Leakage)
Attention : Dans un script pÃ©dagogique, on impute parfois avant le train_test_split pour simplifier. Dans un systÃ¨me industriel, câ€™est une *fuite de donnÃ©es* (Data Leakage).

*   Pourquoi ? Si la moyenne dâ€™une feature est calculÃ©e sur tout le dataset (Train + Test), alors les valeurs du futur jeu de test ont indirectement influencÃ© le nettoyage du Train.
*   La bonne pratique absolue :  
    *   Dâ€™abord sÃ©parer (Train/Test).  
    *   Fit lâ€™imputer sur le Train uniquement.  
    *   Appliquer cette imputation au Test, sans recalculer les statistiques sur le Test.

---

## 4. Analyse Approfondie : Exploration (EDA)

Câ€™est lâ€™Ã©tape de "Profilage" du dataset immobilier.

### DÃ©crypter .describe()
*   *Mean vs 50% (MÃ©diane) :*  
    Pour des variables comme MedInc ou HouseAge, comparer la moyenne et la mÃ©diane permet de voir si la distribution est symÃ©trique ou tirÃ©e par des quartiers trÃ¨s riches / trÃ¨s anciens.
*   *Std (Ã‰cart-type) :*  
    Mesure la dispersion des valeurs : un std Ã©levÃ© indique de fortes diffÃ©rences de revenu ou de densitÃ© entre quartiers, un std trÃ¨s faible signalerait une variable presque constante (peu utile pour le modÃ¨le).

### CorrÃ©lations et Structure Spatiale
En regardant la *heatmap de corrÃ©lation*, on peut observer :

*   Une forte corrÃ©lation positive entre MedInc (revenu mÃ©dian) et la target (prix moyen des maisons), ce qui est intuitif : les zones plus riches ont des logements plus chers.
*   Des liens entre des variables comme AveRooms, AveBedrms et les prix, qui reflÃ¨tent la taille moyenne des logements.
*   Lâ€™effet potentiel de la localisation (Latitude, Longitude) : en combinant ces variables avec la cible, on voit souvent que certaines zones gÃ©ographiques (proche de la cÃ´te, par exemple) ont des prix systÃ©matiquement plus Ã©levÃ©s.

---

## 5. Analyse Approfondie : MÃ©thodologie (Split)

### Le Concept : La Garantie de GÃ©nÃ©ralisation
Le but du modÃ¨le nâ€™est pas de mÃ©moriser les 20 640 Ã©chantillons historiques, mais dâ€™Ãªtre capable de prÃ©dire correctement les prix de logements dans de *nouveaux quartiers*.  
Pour cela, on sÃ©pare les donnÃ©es en Train/Test, et le Test est utilisÃ© uniquement Ã  la toute fin, comme un examen de gÃ©nÃ©ralisation.

### Les ParamÃ¨tres sous le capot
train_test_split(test_size=0.2, random_state=42)

1.  *Le Ratio 80/20 :*  
    *   80 % des donnÃ©es servent Ã  lâ€™entraÃ®nement (le modÃ¨le apprend les patterns â€œprix = f(features)â€).
    *   20 % sont gardÃ©s pour mesurer la performance sur des donnÃ©es jamais vues.

2.  **La ReproductibilitÃ© (random_state) :**  
    Fixer random_state=42 permet dâ€™obtenir toujours la mÃªme sÃ©paration Train/Test, ce qui est essentiel pour comparer les rÃ©sultats entre versions du modÃ¨le ou entre diffÃ©rents algorithmes.

---

## 6. FOCUS THÃ‰ORIQUE : Lâ€™Algorithme Random Forest ğŸŒ²

Pourquoi est-ce lâ€™algorithme "couteau suisse" prÃ©fÃ©rÃ© des Data Scientists pour ce type de donnÃ©es tabulaires (revenu, densitÃ©, localisation, etc.) ?

### A. La Faiblesse de lâ€™Individu (Arbre de DÃ©cision)
Un Arbre de DÃ©cision unique dÃ©coupe lâ€™espace des features en zones et affecte un prix moyen Ã  chaque zone.

*   ProblÃ¨me : Il est *obsessif. Il peut se surâ€‘adapter au bruit dâ€™un quartier trÃ¨s atypique (revenu extrÃªmement haut, prix extrÃªme) et crÃ©er une rÃ¨gle trÃ¨s spÃ©cifique, ce qui conduit Ã  une **haute variance* et des prÃ©dictions instables.

### B. La Force du Groupe (Bagging)

Random Forest signifie "ForÃªt AlÃ©atoire". Il crÃ©e plusieurs dizaines (voire centaines) dâ€™arbres.

1.  *Le Bootstrapping (DiversitÃ© des Ã‰chantillons) :*
    *   Chaque arbre sâ€™entraÃ®ne sur un Ã©chantillon bootstrap diffÃ©rent des quartiers (avec tirage avec remise).
    *   ConsÃ©quence : Chaque arbre a une vision lÃ©gÃ¨rement diffÃ©rente du marchÃ© immobilier californien.

2.  *Feature Randomness (DiversitÃ© des Questions) :*
    *   Ã€ chaque split, un arbre nâ€™a accÃ¨s quâ€™Ã  un sousâ€‘ensemble alÃ©atoire des features (par exemple un sousâ€‘ensemble des 8 variables).
    *   ConsÃ©quence : Certains arbres se spÃ©cialisent davantage sur les aspects gÃ©ographiques (Latitude, Longitude), dâ€™autres sur les variables socioâ€‘dÃ©mographiques (MedInc, Population), ce qui enrichit le panel dâ€™â€œopinionsâ€.

### C. Le Consensus (Vote / Moyenne)

Pour un nouveau quartier :

*   Chaque arbre propose un prix (prÃ©diction de rÃ©gression).
*   Le Random Forest prend la *moyenne* de ces prÃ©dictions.
*   Les erreurs individuelles des arbres (bruit) se compensent, ne laissant que la tendance lourde (le signal).

---

## 7. Analyse Approfondie : 


###  Les MÃ©triques de RÃ©gression

On regarde plusieurs mÃ©triques complÃ©mentaires :

1.  *MSE (Mean Squared Error) :*  
    Moyenne des carrÃ©s des erreurs \((y_{rÃ©el} - y_{prÃ©dit})^2\). TrÃ¨s sensible aux grosses erreurs : un quartier fortement mal estimÃ© pÃ©nalise beaucoup le MSE.

2.  *RMSE (Root Mean Squared Error) :*  
    Racine du MSE, exprimÃ©e dans la mÃªme unitÃ© que la target (centaines de milliers de dollars). Donne un ordre de grandeur de lâ€™erreur moyenne en termes de prix.

3.  *MAE (Mean Absolute Error) :*  
    Moyenne des erreurs absolues \(|y_{rÃ©el} - y_{prÃ©dit}|\). Moins influencÃ©e par les outliers, elle donne une idÃ©e plus robuste de â€œcombienâ€ le modÃ¨le se trompe en moyenne par quartier.

4.  *RÂ² (Coefficient de DÃ©termination) :*  
    Mesure la proportion de la variance des prix expliquÃ©e par le modÃ¨le. Un RÂ² proche de 1 signifie que le modÃ¨le explique bien les diffÃ©rences de prix entre quartiers ; un RÂ² proche de 0 indique un modÃ¨le peu utile.


```python
# 8. Ã‰VALUATION
y_pred = model.predict(X_test)

# MÃ©triques
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"ğŸ¯ RÂ² : {r2:.4f}")
print(f"ğŸ“ RMSE: {rmse:.4f} ($100k)")

# Scatter plot
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel("RÃ©alitÃ© ($100k)"); plt.ylabel("PrÃ©diction ($100k)")
plt.title('RÃ©alitÃ© vs PrÃ©diction')
plt.show()

print("\nğŸ FIN")
```

**InterprÃ©tation** : RMSE=0.5 â‰ˆ erreur moyenne 50k$. RÂ²>0.8 = excellent pour baseline.[2]

***

## 8. Pipeline Complet â€“ SchÃ©ma

```
ğŸ“¥ DonnÃ©es Sales â†’ ğŸ§¹ Imputation â†’ âš–ï¸ Scaling â†’ ğŸ”€ Split â†’ ğŸŒ² RF â†’ ğŸ“Š MÃ©triques
   (5% NaN)       â†’ (mean)      â†’ (StdScaler) â†’ 80/20  â†’ (100 trees) â†’ RÂ²/RMSE
```

***

## 9. RÃ©capitulatif Technique

| Phase | Outil | RÃ©sultat Attendu |
|-------|-------|------------------|
| **Chargement** | `fetch_california_housing()` | (20640, 9) [1] |
| **Nettoyage** | `SimpleImputer(mean)` | 0 NaN |
| **Scaling** | `StandardScaler()` | Î¼=0, Ïƒ=1 par feature |
| **ModÃ¨le** | `RandomForestRegressor(100)` | RÂ² â‰ˆ 0.80-0.85 [5] |
| **Ã‰val** | RMSE en $100k | <0.55 typique |

***
### Conclusion du Projet

Ce rapport montre que la Data Science ne sâ€™arrÃªte pas Ã  model.fit(). Câ€™est une chaÃ®ne de dÃ©cisions cohÃ©rentes oÃ¹ :

*   La comprÃ©hension du mÃ©tier (immobilier, prix, variabilitÃ© entre quartiers) guide le choix du dataset, des features et de la mÃ©thode de validation.
*   Le nettoyage, lâ€™EDA, le split Train/Test et le choix dâ€™un Random Forest robuste sont autant dâ€™Ã©tapes critiques.
*   Lâ€™interprÃ©tation des mÃ©triques (MSE, RMSE, MAE, RÂ²) et des visualisations permet de juger si le modÃ¨le est exploitable pour des applications rÃ©elles (agences, investisseurs, collectivitÃ©s) ou sâ€™il nÃ©cessite des itÃ©rations supplÃ©mentaires.

ch_california_housing(as_frame=True)

df = data.frame

df.rename(columns={'MedHouseVal': 'target'}, inplace=True)

print(f"ğŸ“Š Dataset : {df.shape}")
